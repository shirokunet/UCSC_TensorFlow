{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"optimizer_graph.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sherlock_holmes_shorter.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "training_iters = 100000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(925,)\n"
     ]
    }
   ],
   "source": [
    "def prepdata(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    alllines = []\n",
    "    for line in content:\n",
    "        if line:\n",
    "            alllines.extend(line.strip().split())\n",
    "    # Convert to lower case\n",
    "    alllines = [line.lower() for line in alllines]\n",
    "    alllines = np.array(alllines).reshape([-1, ])\n",
    "    print(alllines.shape)\n",
    "    return alllines\n",
    "    \n",
    "wordarray = prepdata(filename)\n",
    "#print(wordarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475 475\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "dictionary, reverse_dictionary = build_dataset(wordarray)\n",
    "vocab_size = len(dictionary)\n",
    "print(len(dictionary), len(reverse_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4]\n",
      "  [36]\n",
      "  [38]]] 42\n"
     ]
    }
   ],
   "source": [
    "def getdata(wordarray, fdict, rdict, n_input, vocab_size): \n",
    "    offset = random.randint(0, len(wordarray))\n",
    "    end_offset = n_input + 1\n",
    "    # Add some randomness on selection process.\n",
    "    if offset > (len(wordarray)-end_offset):\n",
    "        offset = random.randint(0, n_input+1)\n",
    "    #print(offset)\n",
    "    symbols = [fdict[str(wordarray[i])] for i in range(offset, offset+n_input)]\n",
    "    #print(symbols, [rdict[i] for i in symbols])\n",
    "    symbols = np.array(symbols).reshape([-1, n_input, 1])\n",
    "        \n",
    "    labels_onehot = np.zeros([vocab_size], dtype=np.float)\n",
    "    alabel = wordarray[offset+n_input]\n",
    "    #print(alabel, fdict[str(alabel)])\n",
    "    labels_onehot[fdict[str(alabel)]] = 1.0\n",
    "    labels_onehot = labels_onehot.reshape([1, -1])\n",
    "    return (symbols, labels_onehot)\n",
    "\n",
    "symbols, labels_onehot = getdata(wordarray, dictionary, reverse_dictionary, n_input, vocab_size)\n",
    "print(symbols, np.argmax(labels_onehot))\n",
    "#print(symbols[0][0], [reverse_dictionary[i] for i in symbols[0][0].tolist()])\n",
    "#print(reverse_dictionary[symbols[0][0][0]])\n",
    "#for i in range(100):\n",
    "#    getdata(wordarray, dictionary, reverse_dictionary, n_input, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 36, 38] ['a', 'day', 'should']\n"
     ]
    }
   ],
   "source": [
    "code = np.transpose(symbols[0]).tolist()[0]\n",
    "reverse_code = [reverse_dictionary[i] for i in code]\n",
    "print(code,reverse_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining x and y as placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Generate a 3-element sequence of input values\n",
    "    x = tf.split(x,n_input,1)\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cost, optimizer and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the session \n",
    "This will run the graph and use all the tensors that were previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss: 0.0079, acc: 0.00\n",
      "Iter: 1000, loss: 6.3673, acc: 4.70\n",
      "Iter: 2000, loss: 5.6416, acc: 7.40\n",
      "Iter: 3000, loss: 5.6954, acc: 7.90\n",
      "Iter: 4000, loss: 6.2746, acc: 5.80\n",
      "Iter: 5000, loss: 6.3490, acc: 6.90\n",
      "Iter: 6000, loss: 6.5870, acc: 6.50\n",
      "Iter: 7000, loss: 6.5911, acc: 6.80\n",
      "Iter: 8000, loss: 6.7857, acc: 5.30\n",
      "Iter: 9000, loss: 7.0128, acc: 5.30\n",
      "Iter: 10000, loss: 6.9556, acc: 5.70\n",
      "Iter: 11000, loss: 6.9382, acc: 6.00\n",
      "Iter: 12000, loss: 7.2032, acc: 6.70\n",
      "Iter: 13000, loss: 7.1264, acc: 5.80\n",
      "Iter: 14000, loss: 7.3576, acc: 6.90\n",
      "Iter: 15000, loss: 7.6518, acc: 5.60\n",
      "Iter: 16000, loss: 7.7178, acc: 5.60\n",
      "Iter: 17000, loss: 7.7205, acc: 5.70\n",
      "Iter: 18000, loss: 7.8636, acc: 4.80\n",
      "Iter: 19000, loss: 8.0402, acc: 5.50\n",
      "Iter: 20000, loss: 8.0200, acc: 6.70\n",
      "Iter: 21000, loss: 7.8849, acc: 5.60\n",
      "Iter: 22000, loss: 8.1723, acc: 5.70\n",
      "Iter: 23000, loss: 7.8032, acc: 5.70\n",
      "Iter: 24000, loss: 8.0732, acc: 5.20\n",
      "Iter: 25000, loss: 8.0851, acc: 5.80\n",
      "Iter: 26000, loss: 7.7533, acc: 6.00\n",
      "Iter: 27000, loss: 8.0039, acc: 4.90\n",
      "Iter: 28000, loss: 7.9679, acc: 5.60\n",
      "Iter: 29000, loss: 8.1934, acc: 5.40\n",
      "Iter: 30000, loss: 8.1914, acc: 3.90\n",
      "Iter: 31000, loss: 8.2216, acc: 5.00\n",
      "Iter: 32000, loss: 7.9880, acc: 6.10\n",
      "Iter: 33000, loss: 7.9132, acc: 6.30\n",
      "Iter: 34000, loss: 8.0775, acc: 6.50\n",
      "Iter: 35000, loss: 8.1070, acc: 4.80\n",
      "Iter: 36000, loss: 8.1309, acc: 6.00\n",
      "Iter: 37000, loss: 8.1505, acc: 4.90\n",
      "Iter: 38000, loss: 7.9697, acc: 6.10\n",
      "Iter: 39000, loss: 7.9481, acc: 5.70\n",
      "Iter: 40000, loss: 8.2702, acc: 3.60\n",
      "Iter: 41000, loss: 7.9937, acc: 8.10\n",
      "Iter: 42000, loss: 7.9637, acc: 5.70\n",
      "Iter: 43000, loss: 8.0967, acc: 6.90\n",
      "Iter: 44000, loss: 7.7200, acc: 7.30\n",
      "Iter: 45000, loss: 8.0731, acc: 6.40\n",
      "Iter: 46000, loss: 8.0872, acc: 7.00\n",
      "Iter: 47000, loss: 7.9352, acc: 6.20\n",
      "Iter: 48000, loss: 7.9264, acc: 6.80\n",
      "Iter: 49000, loss: 7.8606, acc: 7.00\n",
      "Iter: 50000, loss: 8.1721, acc: 6.10\n",
      "Iter: 51000, loss: 8.1888, acc: 6.30\n",
      "Iter: 52000, loss: 8.1054, acc: 5.00\n",
      "Iter: 53000, loss: 7.9822, acc: 6.50\n",
      "Iter: 54000, loss: 8.1708, acc: 5.60\n",
      "Iter: 55000, loss: 8.0148, acc: 5.70\n",
      "Iter: 56000, loss: 7.9169, acc: 6.00\n",
      "Iter: 57000, loss: 8.2337, acc: 4.70\n",
      "Iter: 58000, loss: 7.9056, acc: 7.40\n",
      "Iter: 59000, loss: 7.9667, acc: 6.90\n",
      "Iter: 60000, loss: 8.0860, acc: 6.40\n",
      "Iter: 61000, loss: 7.9719, acc: 7.00\n",
      "Iter: 62000, loss: 7.9767, acc: 5.30\n",
      "Iter: 63000, loss: 7.8819, acc: 7.40\n",
      "Iter: 64000, loss: 7.9466, acc: 5.40\n",
      "Iter: 65000, loss: 7.9499, acc: 5.40\n",
      "Iter: 66000, loss: 7.9033, acc: 5.60\n",
      "Iter: 67000, loss: 7.9760, acc: 6.90\n",
      "Iter: 68000, loss: 8.1583, acc: 5.70\n",
      "Iter: 69000, loss: 7.9504, acc: 6.20\n",
      "Iter: 70000, loss: 7.6772, acc: 8.80\n",
      "Iter: 71000, loss: 8.1475, acc: 6.70\n",
      "Iter: 72000, loss: 8.0620, acc: 6.60\n",
      "Iter: 73000, loss: 8.2121, acc: 5.50\n",
      "Iter: 74000, loss: 8.2189, acc: 6.50\n",
      "Iter: 75000, loss: 7.8880, acc: 6.80\n",
      "Iter: 76000, loss: 7.9877, acc: 7.30\n",
      "Iter: 77000, loss: 8.1261, acc: 6.10\n",
      "Iter: 78000, loss: 8.0609, acc: 6.40\n",
      "Iter: 79000, loss: 8.1648, acc: 6.50\n",
      "Iter: 80000, loss: 8.3761, acc: 4.10\n",
      "Iter: 81000, loss: 7.8244, acc: 7.10\n",
      "Iter: 82000, loss: 8.1451, acc: 5.10\n",
      "Iter: 83000, loss: 7.8860, acc: 6.60\n",
      "Iter: 84000, loss: 7.8935, acc: 6.00\n",
      "Iter: 85000, loss: 7.9508, acc: 5.10\n",
      "Iter: 86000, loss: 7.6271, acc: 7.50\n",
      "Iter: 87000, loss: 7.8755, acc: 5.00\n",
      "Iter: 88000, loss: 7.7316, acc: 6.00\n",
      "Iter: 89000, loss: 7.9980, acc: 6.40\n",
      "Iter: 90000, loss: 7.9497, acc: 6.20\n",
      "Iter: 91000, loss: 7.9671, acc: 5.50\n",
      "Iter: 92000, loss: 8.0806, acc: 6.40\n",
      "Iter: 93000, loss: 8.1369, acc: 5.10\n",
      "Iter: 94000, loss: 8.0395, acc: 5.40\n",
      "Iter: 95000, loss: 7.7949, acc: 5.60\n",
      "Iter: 96000, loss: 7.9321, acc: 6.40\n",
      "Iter: 97000, loss: 8.0842, acc: 5.90\n",
      "Iter: 98000, loss: 8.0926, acc: 5.90\n",
      "Iter: 99000, loss: 8.1041, acc: 5.50\n",
      "Iter: 100000, loss: 8.2961, acc: 6.20\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    while step <= training_iters:\n",
    "        symbols, labels_onehot = getdata(wordarray, dictionary, reverse_dictionary, \n",
    "                                         n_input, vocab_size)\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                    feed_dict={x: symbols, y: labels_onehot})\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        if(step%display_step  == 0):\n",
    "            print(\"Iter: %d, loss: %0.4f, acc: %0.2f\"%(\n",
    "                            step, total_loss/display_step, 100.0*total_acc/display_step))\n",
    "            total_loss = 0.0\n",
    "            total_acc = 0.0\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.8.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
